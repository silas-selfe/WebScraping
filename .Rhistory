reticulate::repl_python()
from bs4 import BeautifulSoup
markup = '''<html><body><table class="dgrid-row-table" role="presentation"><tr><td class="dgrid-cell dgrid-cell-padding dgrid-column-selectionHandle field-selectionHandle selection-handle-column" role="gridcell"></td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field0 field-OBJECTID field0" role="gridcell">3</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field1 field-PIDN field1" role="gridcell">001-00-00-001.00</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field2 field-ADDRESS field2" role="gridcell">1096 RIVER RD</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field3 field-ACREAGE field3" role="gridcell">1.740</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field4 field-MAIL_ADD1 field4" role="gridcell">1096 RIVER RD</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field5 field-MAIL_CITY field5" role="gridcell">VILLA HILLS</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field6 field-MAIL_STATE field6" role="gridcell">KY</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field7 field-MAIL_ZIP field7" role="gridcell">41017-4429</td></tr></table></div></body></html>'''
soup = BeautifulSoup(markup, 'html.parser')
div = soup.find('div', id='dgrid_2-row-3')
print(div.string)
url = "https://linkgis.org/mapviewer_development"
url_contents = urllib.request.urlopen(url).read()
import urllib
url_contents = urllib.request.urlopen(url).read()
import requests
url_contents = urllib.request.urlopen(url).read()
soup = bs4.BeautifulSoup(url_contents, "html")
soup = BeautifulSoup(url_contents, "html")
div = soup.find("div", {"id:": "dgrid_2-row-3"})
content = str(div)
View(url_contents)
markup = '''<html><body><table class="dgrid-row-table" role="presentation"><tr><td class="dgrid-cell dgrid-cell-padding dgrid-column-selectionHandle field-selectionHandle selection-handle-column" role="gridcell"></td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field0 field-OBJECTID field0" role="gridcell">3</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field1 field-PIDN field1" role="gridcell">001-00-00-001.00</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field2 field-ADDRESS field2" role="gridcell">1096 RIVER RD</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field3 field-ACREAGE field3" role="gridcell">1.740</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field4 field-MAIL_ADD1 field4" role="gridcell">1096 RIVER RD</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field5 field-MAIL_CITY field5" role="gridcell">VILLA HILLS</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field6 field-MAIL_STATE field6" role="gridcell">KY</td><td class="dgrid-cell dgrid-cell-padding dgrid-column-field7 field-MAIL_ZIP field7" role="gridcell">41017-4429</td></tr></table></div></body></html>'''
soup = BeautifulSoup(markup, 'html.parser')
div = soup.find('div', id='dgrid_2-row-3')
print(div.string)
div = soup.find(id="dgrid_2-row-3")
type = soup.find('div', attrs={"class": "dgrid-row dgrid-row-odd ui-state-default"}).findAll('div')
type = soup.find('div', attrs={"class": "dgrid-row dgrid-row-odd ui-state-default"})
print type[2]
print type[2].find('strong').strong
print type[1].find('strong').strong
print(type[1].find('strong').strong)
type = soup.find('div', attrs={"class": "dgrid-content ui-widget-content"})
print(type[1].find('strong').strong)
table = soup.find('div', attrs={"class": "dgrid-content ui-widget-content"})
url = "https://linkgis.org/mapviewer_development"
content = url.read()
url_contents = urllib.request.urlopen(url).read()
soup = BeautifulSoup(url_contents, "html")
div = soup.find("div", {"id:": "dgrid_2-row-3"})
div = soup.find("div", {"class": "dgrid-content ui-widget-content"})
reticulate::repl_python()
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
tweet = "oh my what was that?"
# preprocess tweet
tweet_words = []
for word in tweet.split(' '):
if word.startswith('@') and len(word) > 1:
word = '@user'
elif word.startswith('http'):
word = 'http'
tweet_words.append(word)
print(tweet_words)
tweet_proc = " ".join(tweet_words)
print(tweet_proc)
roberta = "cardiffnlp/twitter-roberta-base-sentiment"
model = AutoModelForSequenceClassification.from_pretrained(roberta)
import pytorch
import pytorch
model = AutoModelForSequenceClassification.from_pretrained(roberta)
import pytorch
model = AutoModelForSequenceClassification.from_pretrained(roberta)
import torch
model = AutoModelForSequenceClassification.from_pretrained(roberta)
roberta = "cardiffnlp/twitter-roberta-base-sentiment"
import torch
model = AutoModelForSequenceClassification.from_pretrained(roberta)
model = AutoModelForSequenceClassification.from_pretrained(roberta)
tokenizer = AutoTokenizer.from_pretrained(roberta)
model = AutoModelForSequenceClassification.from_pretrained(roberta)
roberta = "cardiffnlp/twitter-roberta-base-sentiment-latest"
import torch
model = AutoModelForSequenceClassification.from_pretrained(roberta)
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from scipy.special import softmax
model = AutoModelForSequenceClassification.from_pretrained(roberta)
from transformers import AutoConfig, AutoModelForSequenceClassification
import numpy as np
from scipy.special import softmax
model = AutoModelForSequenceClassification.from_pretrained(roberta)
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from transformers import AutoConfig, AutoModelForSequenceClassification
import numpy as np
from scipy.special import softmax
tweet = "oh my what was that?"
roberta = "cardiffnlp/twitter-roberta-base-sentiment-latest"
model = AutoModelForSequenceClassification.from_pretrained(roberta)
from transformers import pipeline
sentiment_task = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)
roberta = "cardiffnlp/twitter-roberta-base-sentiment-latest"
import torch
model = AutoModelForSequenceClassification.from_pretrained(roberta)
reticulate::repl_python()
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from transformers import AutoConfig, AutoModelForSequenceClassification
import numpy as np
from scipy.special import softmax
tweet = "oh my what was that?"
# preprocess tweet
tweet_words = []
for word in tweet.split(' '):
if word.startswith('@') and len(word) > 1:
word = '@user'
elif word.startswith('http'):
word = 'http'
tweet_words.append(word)
tweet_proc = " ".join(tweet_words)
print(tweet_proc)
roberta = "cardiffnlp/twitter-roberta-base-sentiment-latest"
model = AutoModelForSequenceClassification.from_pretrained(roberta)
labels = []
encoded_tweet = tokenizer(tweet_proc, return_tensor='pt')
encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')
from transformers import pipeline
sentiment_task = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)
tokenizer = AutoTokenizer.from_pretrained(roberta)
labels = ['Negative', 'Neutral', 'Positive']
encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')
print(encoded_tweet)
output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])
print(output)
output = model(**encoded_tweet)
print(output)
scores = output[0][0].detach().numpy()
print(scores)
scores = softmax(scores)
print(scores)
for i in range(len(scores)):
l = lables[i]
s = scores[i]
print(l, s)
labels = ['Negative', 'Neutral', 'Positive']
for i in range(len(scores)):
l = lables[i]
s = scores[i]
print(l, s)
for i in range(len(scores)):
l = labels[i]
s = scores[i]
print(l, s)
